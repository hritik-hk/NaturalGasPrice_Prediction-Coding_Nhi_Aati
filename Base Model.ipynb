{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2302d1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers import Dense, LSTM, RepeatVector, TimeDistributed, Flatten\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "831339de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set seeds to make the experiment more reproducible.\n",
    "from numpy.random import seed\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "047151d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20-May-87</th>\n",
       "      <td>18.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21-May-87</th>\n",
       "      <td>18.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22-May-87</th>\n",
       "      <td>18.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25-May-87</th>\n",
       "      <td>18.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26-May-87</th>\n",
       "      <td>18.63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Price\n",
       "Date            \n",
       "20-May-87  18.63\n",
       "21-May-87  18.45\n",
       "22-May-87  18.55\n",
       "25-May-87  18.60\n",
       "26-May-87  18.63"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('BrentOilPrices.csv', index_col='Date')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9124f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# values = data['Price'].values.reshape(-1,1)\n",
    "# values = values.astype('float32')\n",
    "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# scaled = scaler.fit_transform(values)\n",
    "# train_size = int(len(scaled) * 0.7)\n",
    "# test_size = len(scaled) - train_size\n",
    "# train, test = scaled[0:train_size,:], scaled[train_size:len(scaled),:]\n",
    "# print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "786a66fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5987 2567\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(data) * 0.7)\n",
    "test_size = len(data) - train_size\n",
    "scaled = data['Price'].values\n",
    "scaled = scaled.reshape(-1, 1)\n",
    "train, test = scaled[0:train_size,:], scaled[train_size:len(scaled),:]\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "639753e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset) - look_back):\n",
    "        a = dataset[i:(i + look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    print(len(dataY))\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e5a2516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5986\n",
      "2566\n"
     ]
    }
   ],
   "source": [
    "look_back = 1\n",
    "X_train, y_train = create_dataset(train, look_back)\n",
    "X_test, y_test = create_dataset(test, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1afe6878",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb8fda0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5986, 1, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7de2aa65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 100)               40800     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,901\n",
      "Trainable params: 40,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d75b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/225\n",
      "60/60 [==============================] - 3s 13ms/step - loss: 32.0088 - val_loss: 70.5765\n",
      "Epoch 2/225\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 26.0651 - val_loss: 62.4585\n",
      "Epoch 3/225\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 19.3849 - val_loss: 55.8675\n",
      "Epoch 4/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 15.2674 - val_loss: 52.3149\n",
      "Epoch 5/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 14.9780 - val_loss: 51.0349\n",
      "Epoch 6/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 13.8664 - val_loss: 49.1052\n",
      "Epoch 7/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 13.1393 - val_loss: 47.2298\n",
      "Epoch 8/225\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 13.0141 - val_loss: 45.8338\n",
      "Epoch 9/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 12.4685 - val_loss: 44.3861\n",
      "Epoch 10/225\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 12.1400 - val_loss: 43.1003\n",
      "Epoch 11/225\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 11.5910 - val_loss: 41.9228\n",
      "Epoch 12/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 11.0028 - val_loss: 40.9189\n",
      "Epoch 13/225\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 10.3097 - val_loss: 39.7347\n",
      "Epoch 14/225\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 9.7592 - val_loss: 38.3925\n",
      "Epoch 15/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 9.4717 - val_loss: 37.1602\n",
      "Epoch 16/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 9.1770 - val_loss: 35.9863\n",
      "Epoch 17/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 8.9135 - val_loss: 34.8888\n",
      "Epoch 18/225\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 8.6421 - val_loss: 33.8240\n",
      "Epoch 19/225\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 8.3907 - val_loss: 32.7984\n",
      "Epoch 20/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 8.1627 - val_loss: 31.8237\n",
      "Epoch 21/225\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 7.9262 - val_loss: 30.8902\n",
      "Epoch 22/225\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 7.7043 - val_loss: 30.0006\n",
      "Epoch 23/225\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 7.4845 - val_loss: 29.1481\n",
      "Epoch 24/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 7.2648 - val_loss: 28.3303\n",
      "Epoch 25/225\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 7.0559 - val_loss: 27.5523\n",
      "Epoch 26/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 6.8635 - val_loss: 26.8220\n",
      "Epoch 27/225\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 6.6544 - val_loss: 26.1275\n",
      "Epoch 28/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 6.4821 - val_loss: 25.4724\n",
      "Epoch 29/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 6.2975 - val_loss: 24.8534\n",
      "Epoch 30/225\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 6.1261 - val_loss: 24.2651\n",
      "Epoch 31/225\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 5.9550 - val_loss: 23.7020\n",
      "Epoch 32/225\n",
      "60/60 [==============================] - 1s 8ms/step - loss: 5.7786 - val_loss: 23.1714\n",
      "Epoch 33/225\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 5.6268 - val_loss: 22.6631\n",
      "Epoch 34/225\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 5.4733 - val_loss: 22.1853\n",
      "Epoch 35/225\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 5.3178 - val_loss: 21.7363\n",
      "Epoch 36/225\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 5.1861 - val_loss: 21.3022\n",
      "Epoch 37/225\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 5.0510 - val_loss: 20.8704\n",
      "Epoch 38/225\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 4.9042 - val_loss: 20.4663\n",
      "Epoch 39/225\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 4.7708 - val_loss: 20.0676\n",
      "Epoch 40/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 4.6496 - val_loss: 19.6728\n",
      "Epoch 41/225\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 4.5070 - val_loss: 19.3147\n",
      "Epoch 42/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 4.4068 - val_loss: 18.9178\n",
      "Epoch 43/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 4.2873 - val_loss: 18.5785\n",
      "Epoch 44/225\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 4.1911 - val_loss: 18.2662\n",
      "Epoch 45/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 4.1034 - val_loss: 17.9759\n",
      "Epoch 46/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 4.0404 - val_loss: 17.6866\n",
      "Epoch 47/225\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 4.0682 - val_loss: 17.4578\n",
      "Epoch 48/225\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 4.0402 - val_loss: 17.2002\n",
      "Epoch 49/225\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 4.0424 - val_loss: 16.8985\n",
      "Epoch 50/225\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 3.9521 - val_loss: 16.6273\n",
      "Epoch 51/225\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 3.8638 - val_loss: 16.3408\n",
      "Epoch 52/225\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 3.7801 - val_loss: 16.0883\n",
      "Epoch 53/225\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 3.7081 - val_loss: 15.8133\n",
      "Epoch 54/225\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 3.6366 - val_loss: 15.5512\n",
      "Epoch 55/225\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 3.5849 - val_loss: 15.3082\n",
      "Epoch 56/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 3.5182 - val_loss: 15.0732\n",
      "Epoch 57/225\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 3.4653 - val_loss: 14.8367\n",
      "Epoch 58/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 3.4168 - val_loss: 14.5974\n",
      "Epoch 59/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 3.3655 - val_loss: 14.3374\n",
      "Epoch 60/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 3.3206 - val_loss: 14.1060\n",
      "Epoch 61/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 3.2611 - val_loss: 13.9102\n",
      "Epoch 62/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 3.2049 - val_loss: 13.6610\n",
      "Epoch 63/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 3.1506 - val_loss: 13.4631\n",
      "Epoch 64/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 3.0987 - val_loss: 13.2453\n",
      "Epoch 65/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 3.0417 - val_loss: 13.0599\n",
      "Epoch 66/225\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 3.0042 - val_loss: 12.8483\n",
      "Epoch 67/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.9610 - val_loss: 12.6502\n",
      "Epoch 68/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.9146 - val_loss: 12.4604\n",
      "Epoch 69/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.8690 - val_loss: 12.2689\n",
      "Epoch 70/225\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 2.8245 - val_loss: 12.0919\n",
      "Epoch 71/225\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 2.7841 - val_loss: 11.9053\n",
      "Epoch 72/225\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 2.7482 - val_loss: 11.7187\n",
      "Epoch 73/225\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 2.7166 - val_loss: 11.5245\n",
      "Epoch 74/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.6870 - val_loss: 11.3817\n",
      "Epoch 75/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.6518 - val_loss: 11.2500\n",
      "Epoch 76/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.6071 - val_loss: 11.0920\n",
      "Epoch 77/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.5670 - val_loss: 10.9451\n",
      "Epoch 78/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.5329 - val_loss: 10.8079\n",
      "Epoch 79/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.4968 - val_loss: 10.6873\n",
      "Epoch 80/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.4574 - val_loss: 10.5898\n",
      "Epoch 81/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.4255 - val_loss: 10.5120\n",
      "Epoch 82/225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 0s 7ms/step - loss: 2.4035 - val_loss: 10.3920\n",
      "Epoch 83/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.3809 - val_loss: 10.2769\n",
      "Epoch 84/225\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 2.3672 - val_loss: 10.1607\n",
      "Epoch 85/225\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.3497 - val_loss: 10.0610\n",
      "Epoch 86/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.3203 - val_loss: 9.9151\n",
      "Epoch 87/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.2964 - val_loss: 9.8402\n",
      "Epoch 88/225\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 2.2755 - val_loss: 9.7564\n",
      "Epoch 89/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.2473 - val_loss: 9.6481\n",
      "Epoch 90/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.2372 - val_loss: 9.5788\n",
      "Epoch 91/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.2360 - val_loss: 9.5222\n",
      "Epoch 92/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.2563 - val_loss: 9.4918\n",
      "Epoch 93/225\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 2.2765 - val_loss: 9.4049\n",
      "Epoch 94/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.2799 - val_loss: 9.3180\n",
      "Epoch 95/225\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.2824 - val_loss: 9.2112\n",
      "Epoch 96/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.2925 - val_loss: 9.1260\n",
      "Epoch 97/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.3282 - val_loss: 9.0620\n",
      "Epoch 98/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.3243 - val_loss: 8.9150\n",
      "Epoch 99/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.3088 - val_loss: 8.8611\n",
      "Epoch 100/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.3100 - val_loss: 8.7914\n",
      "Epoch 101/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.3407 - val_loss: 8.7406\n",
      "Epoch 102/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.3614 - val_loss: 8.6634\n",
      "Epoch 103/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.3504 - val_loss: 8.5472\n",
      "Epoch 104/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.3465 - val_loss: 8.5200\n",
      "Epoch 105/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.3710 - val_loss: 8.4084\n",
      "Epoch 106/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.3635 - val_loss: 8.3424\n",
      "Epoch 107/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.3778 - val_loss: 8.2706\n",
      "Epoch 108/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.4040 - val_loss: 8.1918\n",
      "Epoch 109/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.4155 - val_loss: 8.0993\n",
      "Epoch 110/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.4188 - val_loss: 8.0223\n",
      "Epoch 111/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.4206 - val_loss: 7.9403\n",
      "Epoch 112/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.4386 - val_loss: 7.9245\n",
      "Epoch 113/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.4586 - val_loss: 7.8333\n",
      "Epoch 114/225\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 2.4704 - val_loss: 7.7811\n",
      "Epoch 115/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.5053 - val_loss: 7.6849\n",
      "Epoch 116/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.5197 - val_loss: 7.6249\n",
      "Epoch 117/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.5324 - val_loss: 7.5556\n",
      "Epoch 118/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.5411 - val_loss: 7.4902\n",
      "Epoch 119/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.5680 - val_loss: 7.4217\n",
      "Epoch 120/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.6070 - val_loss: 7.3420\n",
      "Epoch 121/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.6459 - val_loss: 7.2628\n",
      "Epoch 122/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.6752 - val_loss: 7.1945\n",
      "Epoch 123/225\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 2.7021 - val_loss: 7.1263\n",
      "Epoch 124/225\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.7066 - val_loss: 7.0779\n",
      "Epoch 125/225\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 2.7194 - val_loss: 7.0100\n",
      "Epoch 126/225\n",
      "36/60 [=================>............] - ETA: 0s - loss: 1.4396"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=225, batch_size=100,\n",
    "                    validation_data=(X_test, y_test), verbose=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2e9754",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403b80ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X_test)\n",
    "plt.plot(yhat, label='predict')\n",
    "plt.plot(y_test, label='true')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc049c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
